{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "031155b9-c0a0-4499-a9a0-84e3c0617894",
   "metadata": {},
   "outputs": [],
   "source": [
    "running = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46828e6e-7c8e-4128-9039-a8b48a8ce933",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Physics Model\n",
    "First, we define a constants class that stores all of the physical constants that the BSS model uses (following AdEx). The \"default\" object is AdEx, but there are also a couple of simple objects representing the Plateau Potential model (for dendrites) and the LIF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a32ff0-683c-4fae-b3f7-be7550b6c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "class PhysicalProperties:\n",
    "    def __init__(self,\n",
    "        C_m, # membrane capacitance (F). You can think of this as a conversion between current and changes in voltage.\n",
    "        g_leak, # leak conductance (S). Controls how fast the neuron likes to fall back to its leak potential.\n",
    "        V_leak, # leak potential (V). The leak (default) potential of the neuron when it isn't stimulated with current.\n",
    "        delta_exp, # exponential slope factor (V). Controls how quickly the neuron spikes once it reaches the exponential threshold.\n",
    "        V_exp, # exponential threshold (V). Once potential reaches this threshold, it will spike very quickly.\n",
    "        T_w, # adaptation time constant (s). Controls how quickly the neuron adapts to itself.\n",
    "        a, # adaptation scaling factor (S). Controls how strongly the neuron adapts to itself.\n",
    "        b, # spike adaptation amount (A). Controls how much the adaptation increases by for each output spike of the neuron.\n",
    "        V_r, # reset potential (V). After a spike, the neuron is forced to this potential using g_reset for a period of time t_r.\n",
    "        V_th, # hard firing threshold (V). This determines at what potential the neuron actually fires.\n",
    "        t_r, # refractory period (s). The duration for which the neuron is forced to V_r after firing.\n",
    "        T_syn, # synaptic time constant (s). This controls how quickly synapse strength decays over time.\n",
    "        g_reset # reset conductance (S). Controls how fast the neuron jumps to reset potential after firing.\n",
    "    ):\n",
    "        self.C_m = C_m\n",
    "        self.g_leak = g_leak\n",
    "        self.V_leak = V_leak\n",
    "        self.delta_exp = delta_exp\n",
    "        self.V_exp = V_exp\n",
    "        self.T_w = T_w\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.V_r = V_r\n",
    "        self.V_th = V_th\n",
    "        self.t_r = t_r\n",
    "        self.T_syn = T_syn\n",
    "        self.g_reset = g_reset\n",
    "\n",
    "AdEx = PhysicalProperties( # I found these in the 2005 AdEx paper, and I made up t_r, T_syn, and g_reset since they weren't specified.\n",
    "        C_m = 281e-12,\n",
    "        g_leak = 30e-9,\n",
    "        V_leak = -70.6e-3,\n",
    "        delta_exp = 2e-3,\n",
    "        V_exp = -50.4e-3,\n",
    "        T_w = 144e-3,\n",
    "        a = 4e-9,\n",
    "        b = 0.0805e-9,\n",
    "        V_r = -80.6e-3,\n",
    "        V_th = 20e-3,\n",
    "        t_r = 0.04,\n",
    "        T_syn = 1,\n",
    "        g_reset = 1e-7\n",
    ")\n",
    "\n",
    "Plateau_Potential = copy(AdEx)\n",
    "Plateau_Potential.V_th = 0 # This one fires instead of going exponential\n",
    "Plateau_Potential.V_r = 200e-3 # When it fires, potential is actually forced above the threshold before falling back down (plateau)\n",
    "Plateau_Potential.t_r = 0.2 # Dendrite plateaus typically last for a while\n",
    "Plateau_Potential.V_exp = 1000 # These never go exponential, so this is an arbitrary large number\n",
    "\n",
    "LIF = copy(AdEx)\n",
    "LIF.V_th = -50.4e-3 # This also fires instead of going exponential\n",
    "LIF.V_exp = 1000 # Like Plateau_Potential, these never go exponential\n",
    "\n",
    "Passive = copy(AdEx)\n",
    "Passive.V_exp = 1000 # also never go exponential\n",
    "Passive.a = 0 # no adaptation \n",
    "Passive.b = 0 # no adaptation\n",
    "Passive.V_th = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d145de32-155d-4a61-922b-dd5b8367614f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## First Step: Python Class\n",
    "I started with a non-Lava python class to emulate the BSS neuron behavior. It simulates only one neuron (no synapses or components) and doesn't account for things like reset conductance and situations where V_r > V_th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e173c0ae-559d-4543-9d52-1b9da7965b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, physical_properties):\n",
    "        self.pp = physical_properties\n",
    "        self.w = 0 # adaptation current\n",
    "        self.V_m = self.pp.V_leak # membrane voltage\n",
    "        self.I = 0 # input current\n",
    "        self.fired_timestamp = None # last time of fire\n",
    "    \n",
    "    def I_syn(self, t): # current from synapses (not implemented in this class, see Lava class)\n",
    "        return 0\n",
    "    \n",
    "    def I_stim(self, t): # current from other components (also not implemented in this class, see Lava class)\n",
    "        return 0\n",
    "    \n",
    "    def dV_m_dt(self, t): # get the derivative of potential at a given time, depending on current and potential\n",
    "        exponential_component = np.exp((self.V_m - self.pp.V_exp)/self.pp.delta_exp)\n",
    "        return (self.pp.g_leak * (self.pp.V_leak - self.V_m) + \n",
    "                self.pp.g_leak * self.pp.delta_exp * exponential_component + \n",
    "                self.I - self.w) / self.pp.C_m\n",
    "    \n",
    "    def dw_dt(self, t): # get the derivative of the adaptation current at a given time, depending on w and potential\n",
    "        return (self.pp.a * (self.V_m - self.pp.V_leak) - self.w) / self.pp.T_w\n",
    "    \n",
    "    def step(self, t, dt): # take a step of length dt and change the neuron's properties accordingly\n",
    "        self.I = self.I_syn(t) + self.I_stim(t)\n",
    "        dw = self.dw_dt(t) * dt\n",
    "        dV_m = self.dV_m_dt(t) * dt\n",
    "        self.w += dw\n",
    "        \n",
    "        if self.fired_timestamp is not None:\n",
    "            self.V_m = self.pp.V_r\n",
    "            if t - self.fired_timestamp >= self.pp.t_r:\n",
    "                self.fired_timestamp = None\n",
    "        else:\n",
    "            self.V_m += dV_m\n",
    "            if(self.V_m > self.pp.V_th):\n",
    "                self.V_m = self.pp.V_th\n",
    "                self.w += self.pp.b\n",
    "                self.fired_timestamp = t\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe687f-4958-41a3-8285-e47234813789",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## User Input\n",
    "Here is a simple extension of that python class which takes a lambda function as input and applies that as a current over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e93a8145-ac86-410a-9d16-16c94889f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualInputNeuron(Neuron): # fn is a lambda from time (seconds) to current (A)\n",
    "    def __init__(self, physical_properties, fn):\n",
    "        super().__init__(physical_properties)\n",
    "        self.fn = fn\n",
    "    def I_syn(self, t):\n",
    "        return self.fn(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee490fcb-c3d1-4bab-a003-fa427f1245d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Graphing\n",
    "Here we define a view functions to graph voltages, currents, and spike times (plot_spikes was copied from a Lava tutorial). Also, a function to test out the python class above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bd4520b-ff3e-480f-b3d8-16ec261a4328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_voltage(name, times, V_ms, legend = None, min_axis = None, max_axis = None):\n",
    "    plt.plot(times, V_ms)\n",
    "    plt.xlabel('Time (s)')  \n",
    "    plt.ylabel('Membrane Voltage (V)')  \n",
    "    plt.ylim(min_axis, max_axis)\n",
    "    plt.title(f'{name} - Voltage')\n",
    "    if legend is not None:\n",
    "        plt.legend(legend)\n",
    "    plt.show()  \n",
    "\n",
    "def plot_current(name, times, Is, legend = None):\n",
    "    plt.plot(times, Is)\n",
    "    plt.xlabel('Time (s)')  \n",
    "    plt.ylabel('Current (A)')\n",
    "    plt.title(f'{name} - Current')\n",
    "    if legend is not None:\n",
    "        plt.legend(legend)\n",
    "    plt.show()\n",
    "\n",
    "# Given a graph name, time data, voltage data, and current data, make graphs of V and I over time.\n",
    "def plot_voltage_and_current(name, times, V_ms, Is, legend = None, min_voltage_axis = None, max_voltage_axis = None):\n",
    "    plot_voltage(name, times, V_ms, legend, min_voltage_axis, max_voltage_axis)\n",
    "    plot_current(name, times, Is, legend)\n",
    "    \n",
    "\n",
    "# Graph spike data over time (can accept multiple spike trains)\n",
    "def plot_spikes(spikes, legend, colors):\n",
    "    offsets = list(range(1, len(spikes) + 1))\n",
    "    \n",
    "    plt.figure(figsize=(10, 3))\n",
    "    \n",
    "    spikes_plot = plt.eventplot(positions=spikes, \n",
    "                                lineoffsets=offsets,\n",
    "                                linelength=0.9,\n",
    "                                colors=colors)\n",
    "    \n",
    "    plt.title(\"Spike arrival\")\n",
    "    plt.xlabel(\"Milliseconds\")\n",
    "    plt.ylabel(\"Neurons\")\n",
    "    plt.yticks(ticks=offsets, labels=legend)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Test the Neuron class above for a certain amount of time and plot the results\n",
    "def test_neuron(test_name, neuron, timestep, total_time, min_voltage_axis = None, max_voltage_axis = None):\n",
    "    time = 0\n",
    "    V_ms = []\n",
    "    Is = []\n",
    "    times = []\n",
    "    while time <= total_time:\n",
    "        neuron.step(time, timestep)\n",
    "        V_ms.append(neuron.V_m)\n",
    "        times.append(time)\n",
    "        Is.append(neuron.I)\n",
    "        time += timestep\n",
    "    plot_voltage_and_current(test_name, times, V_ms, Is, min_voltage_axis, max_voltage_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e0e331-16bc-41ad-bd48-93ebc05b3cb0",
   "metadata": {},
   "source": [
    "## Testing the Python Class\n",
    "Now we can test the python class, using a standard AdEx neuron and a step function for current input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c76349f-a122-41cd-bed9-ae5eda1cebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we are testing an AdEx neuron that receives a step function of manual current input (see current graph below).\n",
    "# The test runs for a total duration of 1s at 1ms intervals.\n",
    "if running:\n",
    "    test_neuron(\"Test 1\", ManualInputNeuron(AdEx, lambda t: 0.5e-9 if t < 0.2 else 0.8e-9 if t > 0.5 else 0), 0.001, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033096a0-6f53-429a-ad90-23f87a34fbd6",
   "metadata": {},
   "source": [
    "## Lava Process\n",
    "Here is the BSS Neuron Process for Lava. As you can see, it taks many variables, since Lava doesn't have the concept of structs, objects, or references. The first section of variables corresponds to physical constants, the second section corresponds to weights/conductances, the third section corresponds to state variables like time, current, and voltage, and the fourth section has all of the IO ports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b522d837-dec8-4f7c-a377-562c2fc8d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lava.magma.core.process.process import AbstractProcess\n",
    "from lava.magma.core.process.variable import Var\n",
    "from lava.magma.core.process.ports.ports import InPort, OutPort, RefPort\n",
    "import numpy as np\n",
    "from lava.magma.core.sync.protocols.loihi_protocol import LoihiProtocol\n",
    "from lava.magma.core.model.py.ports import PyInPort, PyOutPort, PyRefPort\n",
    "from lava.magma.core.model.py.type import LavaPyType\n",
    "from lava.magma.core.resources import CPU\n",
    "from lava.magma.core.decorator import implements, requires, tag\n",
    "from lava.magma.core.model.py.model import PyLoihiProcessModel\n",
    "from lava.magma.core.run_configs import Loihi1SimCfg\n",
    "from lava.magma.core.run_conditions import RunSteps\n",
    "\n",
    "# This class represents an arbitrary number of neurons, but they all share the same physical properties.\n",
    "# For example, it could represent a layer of soma or a layer of a specific type of dendrite.\n",
    "class BSSNeuronProcess(AbstractProcess):\n",
    "    def __init__(self, dt, shape = (1,), name = 'BSSNeuronProcess',\n",
    "        physical_properties = AdEx\n",
    "    ):\n",
    "        super().__init__(name=name)\n",
    "        # See abstract model class below for documentation on properties\n",
    "        self.C_m = Var(shape=(1,), init=physical_properties.C_m)\n",
    "        self.g_leak = Var(shape=(1,), init=physical_properties.g_leak)\n",
    "        self.V_leak = Var(shape=(1,), init=physical_properties.V_leak)\n",
    "        self.delta_exp = Var(shape=(1,), init=physical_properties.delta_exp)\n",
    "        self.V_exp = Var(shape=(1,), init=physical_properties.V_exp)\n",
    "        self.T_w = Var(shape=(1,), init=physical_properties.T_w)\n",
    "        self.a = Var(shape=(1,), init=physical_properties.a)\n",
    "        self.b = Var(shape=(1,), init=physical_properties.b)\n",
    "        self.V_r = Var(shape=(1,), init=physical_properties.V_r)\n",
    "        self.V_th = Var(shape=(1,), init=physical_properties.V_th)\n",
    "        self.t_r = Var(shape=(1,), init=physical_properties.t_r)\n",
    "        self.T_syn = Var(shape=(1,), init=physical_properties.T_syn)\n",
    "        self.g_reset = Var(shape=(1,), init=physical_properties.g_reset)\n",
    "        \n",
    "        self.g_above = Var(shape=shape, init=0)\n",
    "        self.g_left = Var(shape=shape, init=0)\n",
    "        self.g_right = Var(shape=shape, init=0)\n",
    "        self.g_below = Var(shape=shape, init=0)\n",
    "        \n",
    "        self.dt = Var(shape=(1,), init=dt)\n",
    "        self.I = Var(shape=shape, init=0)\n",
    "        self.w = Var(shape=shape, init=0)\n",
    "        self.V_m = Var(shape=shape, init=physical_properties.V_leak)\n",
    "        self.t = Var(shape=(1,), init=0)\n",
    "        self.fired_timestamp = Var(shape=shape, init=-1)\n",
    "        \n",
    "        self.spike_out = OutPort(shape=shape)\n",
    "        self.synapse_in = InPort(shape=shape)\n",
    "        self.V_m_out = OutPort(shape=shape)\n",
    "        self.above_in = InPort(shape=shape)\n",
    "        self.left_in = InPort(shape=shape)\n",
    "        self.right_in = InPort(shape=shape)\n",
    "        self.below_in = InPort(shape=shape)\n",
    "    def update_physical_properties(self, physical_properties):\n",
    "        self.C_m.init = physical_properties.C_m\n",
    "        self.g_leak.init = physical_properties.g_leak\n",
    "        self.V_leak.init = physical_properties.V_leak\n",
    "        self.delta_exp.init = physical_properties.delta_exp\n",
    "        self.V_exp.init = physical_properties.V_exp\n",
    "        self.T_w.init = physical_properties.T_w\n",
    "        self.a.init = physical_properties.a\n",
    "        self.b.init = physical_properties.b\n",
    "        self.V_r.init = physical_properties.V_r\n",
    "        self.V_th.init = physical_properties.V_th\n",
    "        self.t_r.init = physical_properties.t_r\n",
    "        self.T_syn.init = physical_properties.T_syn\n",
    "        self.g_reset.init = physical_properties.g_reset\n",
    "        self.V_m.init = physical_properties.V_leak\n",
    "\n",
    "from enum import Enum\n",
    "class Direction(Enum):\n",
    "    ABOVE = 0\n",
    "    LEFT = 1\n",
    "    RIGHT = 2\n",
    "    BELOW = 3\n",
    "\n",
    "def get_corresponding_in_port(neuron, direction):\n",
    "    if direction == Direction.ABOVE:\n",
    "        return neuron.above_in\n",
    "    if direction == Direction.LEFT:\n",
    "        return neuron.left_in\n",
    "    if direction == Direction.RIGHT:\n",
    "        return neuron.right_in\n",
    "    if direction == Direction.BELOW:\n",
    "        return neuron.below_in\n",
    "    assert False, \"unexpected direction.\"\n",
    "\n",
    "def get_corresponding_conductance(neuron, direction):\n",
    "    if direction == Direction.ABOVE:\n",
    "        return neuron.g_above\n",
    "    if direction == Direction.LEFT:\n",
    "        return neuron.g_left\n",
    "    if direction == Direction.RIGHT:\n",
    "        return neuron.g_right\n",
    "    if direction == Direction.BELOW:\n",
    "        return neuron.g_below\n",
    "    assert False, \"unexpected direction.\"\n",
    "\n",
    "def get_in_direction(out_direction):\n",
    "    if out_direction == Direction.ABOVE:\n",
    "        return Direction.BELOW\n",
    "    if out_direction == Direction.LEFT:\n",
    "        return Direction.RIGHT\n",
    "    if out_direction == Direction.RIGHT:\n",
    "        return Direction.LEFT\n",
    "    if out_direction == Direction.BELOW:\n",
    "        return Direction.ABOVE\n",
    "\n",
    "def connect_neurons(first, second, out_direction, g):\n",
    "    first_in_port = get_corresponding_in_port(first, out_direction)\n",
    "    second_in_port = get_corresponding_in_port(second, get_in_direction(out_direction))\n",
    "    assert (not first_in_port.in_connections) and (not second_in_port.in_connections), \"local neuron ports can only have one connection!\"\n",
    "    \n",
    "    first.V_m_out.connect(get_corresponding_in_port(second, get_in_direction(out_direction)))\n",
    "    second.V_m_out.connect(get_corresponding_in_port(first, out_direction))\n",
    "\n",
    "    get_corresponding_conductance(first, out_direction).init = g\n",
    "    get_corresponding_conductance(second, get_in_direction(out_direction)).init = g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c87125-7085-43f5-a825-9cc005a7302f",
   "metadata": {},
   "source": [
    "## Lava Model\n",
    "Here is the model implementation of the process above. I made it \"abstract\" (i.e. not technically an implementation) so that it can serve as a parent class to more specific implementations. As you can see, it stores the same variables as the process (required by Lava). See function documentation for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af0088af-5036-4c71-b522-3e467c18551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractBSSNeuronProcessModel(PyLoihiProcessModel):\n",
    "    C_m: float = LavaPyType(float, float) # These are the same physical properties as in the PhysicalProperties class above\n",
    "    g_leak: float = LavaPyType(float, float)\n",
    "    V_leak: float = LavaPyType(float, float)\n",
    "    delta_exp: float = LavaPyType(float, float)\n",
    "    V_exp: float = LavaPyType(float, float)\n",
    "    T_w: float = LavaPyType(float, float)\n",
    "    a: float = LavaPyType(float, float)\n",
    "    b: float = LavaPyType(float, float)\n",
    "    V_r: float = LavaPyType(float, float)\n",
    "    V_th: float = LavaPyType(float, float)\n",
    "    t_r: float = LavaPyType(float, float)\n",
    "    T_syn: float = LavaPyType(float, float)\n",
    "    g_reset: float = LavaPyType(float, float)\n",
    "    \n",
    "    g_above: np.ndarray = LavaPyType(np.ndarray, float) # The conductance (weight) for this neuron's optional connection to a compartment above\n",
    "    g_left: np.ndarray = LavaPyType(np.ndarray, float) # The conductance (weight) for this neuron's optional connection to a compartment on the left\n",
    "    g_right: np.ndarray = LavaPyType(np.ndarray, float) # The conductance (weight) for this neuron's optional connection to a compartment on the right\n",
    "    g_below: np.ndarray = LavaPyType(np.ndarray, float) # The conductance (weight) for this neuron's optional connection to a compartment below\n",
    "    \n",
    "    dt: float = LavaPyType(float, float) # The time interval for this neuron to run at\n",
    "    I: np.ndarray = LavaPyType(np.ndarray, float) # The sum of manual currents + compartment currents + synapse currents, used for logging\n",
    "    w: np.ndarray = LavaPyType(np.ndarray, float) # The adaptation currents of this neuron layer\n",
    "    V_m: np.ndarray = LavaPyType(np.ndarray, float) # The membrane voltages of this neuron layer\n",
    "    t: float = LavaPyType(float, float) # The current time\n",
    "    fired_timestamp: np.ndarray = LavaPyType(np.ndarray, float) # A list of timestamps for when each neuron in the layer last fired\n",
    "    \n",
    "    spike_out: PyOutPort = LavaPyType(PyOutPort.VEC_DENSE, bool, precision=1) # Send a list of bools where each True represents a fire at that index\n",
    "    synapse_in: PyInPort = LavaPyType(PyInPort.VEC_DENSE, float) # Receives a list of the form sent by synapse_out\n",
    "    V_m_out: PyOutPort = LavaPyType(PyOutPort.VEC_DENSE, float) # sends all potentials from this neuron layer\n",
    "    above_in: PyInPort = LavaPyType(PyInPort.VEC_DENSE, float) # Connects to a V_m_out of another neuron layer\n",
    "    left_in: PyInPort = LavaPyType(PyInPort.VEC_DENSE, float) # Connects to a V_m_out of another neuron layer\n",
    "    right_in: PyInPort = LavaPyType(PyInPort.VEC_DENSE, float) # Connects to a V_m_out of another neuron layer\n",
    "    below_in: PyInPort = LavaPyType(PyInPort.VEC_DENSE, float) # Connects to a V_m_out of another neuron layer\n",
    "\n",
    "    # I_MC is updated based on the difference between the membrane voltage of each connection and this neuron's voltage\n",
    "    def I_MC(self, t):\n",
    "        total = np.zeros(np.shape(self.I))\n",
    "        if t == 0:\n",
    "            return total # can't expect anything on first step\n",
    "        for port, conductance in ((self.above_in, self.g_above), \n",
    "                                  (self.left_in, self.g_left), \n",
    "                                  (self.right_in, self.g_right), \n",
    "                                  (self.below_in, self.g_below)):\n",
    "            if not port.csp_ports:\n",
    "                continue # if port isn't connected\n",
    "            total += np.multiply(conductance, (port.recv() - self.V_m))\n",
    "        return total\n",
    "        \n",
    "    # Get the currents in this layer based on synapses, using the synapse weight matrix\n",
    "    # This current deteriorates exponentialfly over time (given in BSS paper), don't really understand why\n",
    "    def I_syn(self, t):\n",
    "        if not self.synapse_in.csp_ports or t == 0:\n",
    "            return np.zeros(np.shape(self.I)) # if neuron doesn't receive from synapse, also can't expect anything on first step\n",
    "        weighted_in = self.synapse_in.recv()\n",
    "        return weighted_in * np.exp(-t / self.T_syn)\n",
    "\n",
    "    # Get the current from user input (overridden in child classes)\n",
    "    def I_stim(self, t):\n",
    "        return np.zeros(np.shape(self.I))\n",
    "\n",
    "    # Get the derivative of membrane voltage, based on current and voltage\n",
    "    def dV_m_dt(self):\n",
    "        exponential_component = np.exp((self.V_m - self.V_exp)/self.delta_exp)\n",
    "        return (self.g_leak * (self.V_leak - self.V_m) + \n",
    "                self.g_leak * self.delta_exp * exponential_component + \n",
    "                self.I - self.w) / self.C_m\n",
    "\n",
    "    # Get the derivative of the adaptation current, based on w and voltage\n",
    "    def dw_dt(self):\n",
    "        return (self.a * (self.V_m - self.V_leak) - self.w) / self.T_w\n",
    "\n",
    "    # Update the neuron layer\n",
    "    def run_spk(self):\n",
    "        which_fired = self.fired_timestamp >= 0 # Check which neurons recently fired\n",
    "        which_are_done_resting = np.logical_and(which_fired, (self.t - self.fired_timestamp >= self.t_r)) # Check which of those should stop resting\n",
    "        self.fired_timestamp[which_are_done_resting] = -1 # Tell those ones to stop resting\n",
    "\n",
    "        \n",
    "        self.I = self.I_syn(self.t) + self.I_MC(self.t) + self.I_stim(self.t) # Calculate current \n",
    "        dw_dt = self.dw_dt() # Get change in adaptation current\n",
    "        dV_m_dt = np.where(which_fired, (self.g_reset * (self.V_r - self.V_m))/self.C_m, self.dV_m_dt()) # Get change in potential\n",
    "        # ^ Note that the neurons in the layer which fired recently will behave differently, clamping to V_r instead of being affected by dV_m_dt\n",
    "        \n",
    "        self.w += dw_dt * self.dt # Change adaptation current based on derivative\n",
    "        which_below_threshold = self.V_m < self.V_th # Check which neurons in the layer are below firing threshold\n",
    "        self.V_m += dV_m_dt * self.dt # Update potential\n",
    "        which_above_threshold = self.V_m >= self.V_th # Check which neurons in the layer are above firing threshold\n",
    "\n",
    "        \n",
    "        which_should_fire = np.logical_and(which_below_threshold, which_above_threshold) # If a neuron transitioned from below to above, it should fire\n",
    "        self.w[which_should_fire] += self.b # Increase adaptation currents for firing neurons\n",
    "        self.V_m[which_should_fire] = self.V_th # Clamp firing neuron potentials to V_th so they don't get out of control\n",
    "        self.fired_timestamp[which_should_fire] = self.t # Update timestamps\n",
    "        self.spike_out.send(which_should_fire) # Send spike signal\n",
    "        self.V_m_out.send(self.V_m)\n",
    "        self.t += self.dt # Increment time counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fe143d-82b0-4981-9ef3-6890a8f1962f",
   "metadata": {},
   "source": [
    "## Child Lava Models\n",
    "Here are some more child classes building off of the process/model written above. The first model is exactly the same as the abstract process model, except that it's no longer abstract. The next process/model pair written here can be used to feed user input as current into neurons. Unfortunately Lava has no concept of lambda functions, so if you want behavior that's not a simple step function, you'll have to directly edit this model or make a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2687a4a7-d84e-469a-bc5f-277dda28303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An exact copy of AbstractBSSNeuronProcessModel\n",
    "@implements(proc=BSSNeuronProcess, protocol=LoihiProtocol)\n",
    "@requires(CPU)\n",
    "class BSSNeuronProcessModel(AbstractBSSNeuronProcessModel):\n",
    "    pass\n",
    "\n",
    "# This class accepts a step function as input (can vary per neuron) and will apply that to the layer\n",
    "class BSSNeuronProcessManualInput(BSSNeuronProcess):\n",
    "    def __init__(self, dt, shape = (1,), name = 'BSSNeuronProcessManualInput', physical_properties = AdEx,\n",
    "                 times = (), heights = ()):\n",
    "        super().__init__(dt=dt, shape=shape, name=name, physical_properties=physical_properties)\n",
    "        self.heights = Var(shape = np.shape(heights), init=heights)\n",
    "        self.times = Var(shape= np.shape(times), init=times)\n",
    "\n",
    "# A model implementing the process above\n",
    "@implements(proc=BSSNeuronProcessManualInput, protocol=LoihiProtocol)\n",
    "@requires(CPU)\n",
    "class BSSNeuronProcessModelManualInput(AbstractBSSNeuronProcessModel):\n",
    "    times: np.ndarray = LavaPyType(np.ndarray, float)\n",
    "    heights: np.ndarray = LavaPyType(np.ndarray, float)\n",
    "    \n",
    "    def I_stim(self, t):\n",
    "        heights = np.zeros(np.shape(self.I))\n",
    "        for i, time in enumerate(self.times):\n",
    "            heights = np.where(t >= time, self.heights[i], heights)\n",
    "        return heights\n",
    "\n",
    "# This class accepts a step function as input (can vary per neuron) and will apply that to the layer\n",
    "class BSSNeuronProcessTriangleSpike(BSSNeuronProcess):\n",
    "    def __init__(self, dt, shape = (1,), name = 'BSSNeuronProcessManualInput', physical_properties = AdEx,\n",
    "                 starts = 0, heights = 0, uptimes = 1, downtimes = 1):\n",
    "        super().__init__(dt=dt, shape=shape, name=name, physical_properties=physical_properties)\n",
    "        self.starts = Var(shape = shape, init=starts)\n",
    "        self.heights = Var(shape = shape, init=heights)\n",
    "        self.uptimes = Var(shape = shape, init=uptimes)\n",
    "        self.downtimes = Var(shape = shape, init=downtimes)\n",
    "\n",
    "# A model implementing the process above\n",
    "@implements(proc=BSSNeuronProcessTriangleSpike, protocol=LoihiProtocol)\n",
    "@requires(CPU)\n",
    "class BSSNeuronProcessModelTriangleSpike(AbstractBSSNeuronProcessModel):\n",
    "    starts: np.ndarray = LavaPyType(np.ndarray, float)\n",
    "    heights: np.ndarray = LavaPyType(np.ndarray, float)\n",
    "    uptimes: np.ndarray = LavaPyType(np.ndarray, float)\n",
    "    downtimes: np.ndarray = LavaPyType(np.ndarray, float)\n",
    "    \n",
    "    def I_stim(self, t):\n",
    "        t_heights = np.zeros(np.shape(self.I))\n",
    "        \n",
    "        which_going_up = np.logical_and(t > self.starts, t <= self.starts + self.uptimes)\n",
    "        t_heights = np.where(which_going_up, np.multiply(self.heights, np.divide(t - self.starts, self.uptimes)), t_heights)\n",
    "        \n",
    "        which_going_down = np.logical_and(t > self.starts + self.uptimes, t <= self.starts + self.uptimes + self.downtimes)\n",
    "        t_heights = np.where(which_going_down, np.multiply(self.heights, 1 - np.divide(t - (self.starts + self.uptimes), self.downtimes)), t_heights)\n",
    "        \n",
    "        return t_heights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6588b80-bd09-4b35-93ac-7ee39bf18eef",
   "metadata": {},
   "source": [
    "## Synapse processes and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc743baa-45e5-415f-a10c-3258926de0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSSSynapseProcess(AbstractProcess):\n",
    "    def __init__(self, in_shape = (1,), out_shape=(1,), name = 'BSSSynapseProcess',\n",
    "        synapse_weights = 0\n",
    "    ):\n",
    "        super().__init__(name=name)\n",
    "        self.synapse_weights = Var(shape=out_shape+in_shape, init=synapse_weights)\n",
    "        \n",
    "        self.synapse_in = InPort(shape=in_shape)\n",
    "        self.synapse_out = OutPort(shape=out_shape)\n",
    "\n",
    "class AbstractBSSSynapseProcessModel(PyLoihiProcessModel):\n",
    "    synapse_weights: np.ndarray = LavaPyType(np.ndarray, float) # A weight matrix for synapses, used to connect this layer of neurons\n",
    "                                                                # to another layer of the same shape (or it can feed into itself, but this would be rare)\n",
    "    synapse_in: PyInPort = LavaPyType(PyInPort.VEC_DENSE, bool, precision=1)\n",
    "    synapse_out: PyOutPort = LavaPyType(PyOutPort.VEC_DENSE, float)\n",
    "\n",
    "    def run_spk(self):\n",
    "        if self.synapse_in.csp_ports and self.synapse_out.csp_ports:\n",
    "            in_results = self.synapse_in.recv()\n",
    "            self.synapse_out.send(self.synapse_weights @ in_results)\n",
    "\n",
    "@implements(proc=BSSSynapseProcess, protocol=LoihiProtocol)\n",
    "@requires(CPU)\n",
    "class BSSSynapseProcessModel(AbstractBSSSynapseProcessModel):\n",
    "    pass\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716fad9c-7b22-48b7-8363-c46f4b86f487",
   "metadata": {},
   "source": [
    "## Monitoring Classes\n",
    "These classes just make it a little easier to monitor process data using Lava Monitor objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e98803ef-f01e-44a4-bb03-2b1ba52e71f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lava.proc.monitor.process import Monitor\n",
    "\n",
    "# Wraps a Monitor object\n",
    "class NeuronMonitor:\n",
    "    def __init__(self, neuron_name, var, var_name, num_steps):\n",
    "        self.monitor = Monitor()\n",
    "        self.monitor.probe(var, num_steps)\n",
    "        self.neuron_name = neuron_name\n",
    "        self.var_name = var_name\n",
    "    def get_neuron_data(self):\n",
    "        return self.monitor.get_data()[self.neuron_name][self.var_name]\n",
    "\n",
    "# Wraps a few NeuronMonitor objects that are commonly used\n",
    "class StandardNeuronMonitors:\n",
    "    def __init__(self, neuron, num_steps):\n",
    "        self.V_m_monitor = NeuronMonitor(neuron.name, neuron.V_m, 'V_m', num_steps)\n",
    "        self.I_monitor = NeuronMonitor(neuron.name, neuron.I, 'I', num_steps)\n",
    "        self.spike_monitor = NeuronMonitor(neuron.name, neuron.spike_out, 'spike_out', num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc451109-bdd5-4b9f-9808-971a5c002caa",
   "metadata": {},
   "source": [
    "## AdEx Reacting to Step Function\n",
    "This experiment is designed to imitate Figure 2C in 'Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity', 2005, by Brette and Gerstner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d85b677-df53-46eb-9a0b-1eb0efb2d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if running:\n",
    "    total_time = 1\n",
    "    dt = 0.001\n",
    "    dendrites_name = 'DendriteProcess'\n",
    "    num_neurons = 1\n",
    "    shape = (num_neurons,)\n",
    "    model_tag = None\n",
    "    \n",
    "    num_steps = (int)(total_time // dt)\n",
    "    \n",
    "    dendrites = BSSNeuronProcessManualInput(name=dendrites_name, dt=dt, shape=shape,\n",
    "                                # since no physics was specified, this will be an AdEx neuron layer\n",
    "                                heights = (\n",
    "                                    0.5e-9, 0, 0.8e-9 # step function as shown in the paper\n",
    "                                ), times = (\n",
    "                                    0, 0.2, 0.5\n",
    "                                ))\n",
    "    \n",
    "    dendrites_monitors = StandardNeuronMonitors(dendrites, num_steps)\n",
    "    \n",
    "    run_cfg = Loihi1SimCfg(select_tag=model_tag)\n",
    "    dendrites.run(condition=RunSteps(num_steps=num_steps), run_cfg=run_cfg)\n",
    "    \n",
    "    plot_voltage_and_current('AdEx Neuron 1', [i * dt for i in range(num_steps)], \n",
    "                             dendrites_monitors.V_m_monitor.get_neuron_data(), \n",
    "                             dendrites_monitors.I_monitor.get_neuron_data())\n",
    "    \n",
    "    dendrites.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677bf305-4a1e-411d-bf04-06c36eb42676",
   "metadata": {},
   "source": [
    "## AdEx Reacting to Current Spike\n",
    "This experiment is meant to imitate Figure 2A in 'Emulating Dendritic Computing Paradigms On Analog Neuromorphic Hardware', 2022, by Kaiser et al (the paper that describes the BSS-2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b738702f-2950-4441-8278-20161802b1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if running:\n",
    "    total_time = 0.25\n",
    "    dt = 0.001\n",
    "    dendrites_name = 'DendriteProcess'\n",
    "    num_neurons = 1\n",
    "    shape = (num_neurons,)\n",
    "    model_tag = None\n",
    "    \n",
    "    num_steps = (int)(total_time // dt)\n",
    "    \n",
    "    dendrites = BSSNeuronProcessManualInput(name=dendrites_name, dt=dt, shape=shape,\n",
    "                                physical_properties = AdEx, # this neuron explicitly tells us that it's an AdEx layer\n",
    "                                heights = (\n",
    "                                    0.8e-9, 0 # this step function represents a very short current spike\n",
    "                                ), times = (\n",
    "                                    0.01, 0.03\n",
    "                                ))\n",
    "    \n",
    "    dendrites_monitors = StandardNeuronMonitors(dendrites, num_steps)\n",
    "    \n",
    "    run_cfg = Loihi1SimCfg(select_tag=model_tag)\n",
    "    dendrites.run(condition=RunSteps(num_steps=num_steps), run_cfg=run_cfg)\n",
    "    \n",
    "    plot_voltage_and_current('AdEx Neuron 2', [i * dt for i in range(num_steps)], \n",
    "                             dendrites_monitors.V_m_monitor.get_neuron_data(), \n",
    "                             dendrites_monitors.I_monitor.get_neuron_data())\n",
    "    \n",
    "    dendrites.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9885344-5217-4090-9775-e361d10199d1",
   "metadata": {},
   "source": [
    "## LIF Reacting to Current Spike\n",
    "This experiment is meant to imitate Figure 2B in 'Emulating Dendritic Computing Paradigms On Analog Neuromorphic Hardware', 2022, by Kaiser et al (the paper that describes the BSS-2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18095389-c019-4cbf-a289-cb5980d7d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if running:\n",
    "    total_time = 0.25\n",
    "    dt = 0.001\n",
    "    dendrites_name = 'DendriteProcess'\n",
    "    num_neurons = 1\n",
    "    shape = (num_neurons,)\n",
    "    model_tag = None\n",
    "    \n",
    "    num_steps = (int)(total_time // dt)\n",
    "    \n",
    "    dendrites = BSSNeuronProcessManualInput(name=dendrites_name, dt=dt, shape=shape,\n",
    "                                physical_properties = LIF, # note this important different - this is now a LIF neuron layer\n",
    "                                heights = (\n",
    "                                    0.8e-9, 0 # the same step function as above, a short current spike\n",
    "                                ), times = (\n",
    "                                    0.01, 0.03\n",
    "                                ))\n",
    "    \n",
    "    dendrites_monitors = StandardNeuronMonitors(dendrites, num_steps)\n",
    "    \n",
    "    run_cfg = Loihi1SimCfg(select_tag=model_tag)\n",
    "    dendrites.run(condition=RunSteps(num_steps=num_steps), run_cfg=run_cfg)\n",
    "    \n",
    "    plot_voltage_and_current('LIF Neuron', [i * dt for i in range(num_steps)], \n",
    "                             dendrites_monitors.V_m_monitor.get_neuron_data(), \n",
    "                             dendrites_monitors.I_monitor.get_neuron_data())\n",
    "    \n",
    "    dendrites.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8cbd41-f0d7-42a3-a7b9-74a7279cec96",
   "metadata": {},
   "source": [
    "## Plateau Potential Reacting to Current Spike\n",
    "This experiment is meant to imitate Figure 2C in 'Emulating Dendritic Computing Paradigms On Analog Neuromorphic Hardware', 2022, by Kaiser et al (the paper that describes the BSS-2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "395a333d-3447-42f7-9fe5-8d1e09d01ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if running:\n",
    "    total_time = 0.25\n",
    "    dt = 0.001\n",
    "    dendrites_name = 'DendriteProcess'\n",
    "    num_neurons = 1\n",
    "    shape = (num_neurons,)\n",
    "    model_tag = None\n",
    "    \n",
    "    num_steps = (int)(total_time // dt)\n",
    "    \n",
    "    dendrites = BSSNeuronProcessManualInput(name=dendrites_name, dt=dt, shape=shape,\n",
    "                                physical_properties = Plateau_Potential, # this is a Plateau Potential neuron layer\n",
    "                                heights = (\n",
    "                                    2.5e-9, 0 # same step function as before\n",
    "                                ), times = (\n",
    "                                    0.01, 0.03\n",
    "                                ))\n",
    "    \n",
    "    dendrites_monitors = StandardNeuronMonitors(dendrites, num_steps)\n",
    "    \n",
    "    run_cfg = Loihi1SimCfg(select_tag=model_tag)\n",
    "    dendrites.run(condition=RunSteps(num_steps=num_steps), run_cfg=run_cfg)\n",
    "    \n",
    "    plot_voltage_and_current('Plateau Potential Neuron', [i * dt for i in range(num_steps)], \n",
    "                             dendrites_monitors.V_m_monitor.get_neuron_data(), \n",
    "                             dendrites_monitors.I_monitor.get_neuron_data())\n",
    "    \n",
    "    dendrites.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b42d1-5449-4a84-a529-0315163b2734",
   "metadata": {},
   "source": [
    "## Synapses and Compartments\n",
    "Here is an experiment with two neurons, each with two compartments. The series of events is as follows:\n",
    "1) A short spike of current is injected into neuron 1 compartment 1, a Plateau Potential dendrite\n",
    "2) This produces a voltage plateau potential in the dendrite\n",
    "3) The second compartment, an AdEx soma, registers the dendrite as a connected compartment, so it reads off the dendrite's voltage, producing a current in the soma\n",
    "4) This produces a voltage in the soma, causing the neuron to spike\n",
    "5) Neuron 2 compartment 1, another Plateau Potential dendrite, is connected to neuron 1 compartment 2 (think of it as an axon) via synapse, so it receives current from the spike\n",
    "6) This produces a voltage plateau potential in the second dendrite\n",
    "7) The second compartment of neuron 2 is connected to the first compartment, so it reads off the voltage and produces a current in the soma of neuron 2\n",
    "8) This produces a voltage in the soma of neuron 2, causing the neuron to spike\n",
    "   \n",
    "Overall, we go from a manual current input on a dendrite on one neuron to a spike from the axon of another neuron. Actually, since the plateau potentials last for so long, the initial short spike of current turns into three output spikes per neuron, as you'll see in the graphs. The blue curves correspond to the first neuron, and the orange curves correspond to the second neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54e23461-be0a-4547-ac6e-b7d40840abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if running:\n",
    "    total_time = 1\n",
    "    dt = 0.001\n",
    "    dendrites_name = 'DendriteProcess'\n",
    "    somas_name = 'SomaProcess'\n",
    "    num_neurons = 2 # note that we now have 2 neurons per layer!\n",
    "    shape = (num_neurons,)\n",
    "    model_tag = None\n",
    "    \n",
    "    num_steps = (int)(total_time // dt)\n",
    "    \n",
    "    dendrites = BSSNeuronProcessManualInput(name=dendrites_name, dt=dt, shape=shape,\n",
    "                                physical_properties=Plateau_Potential,\n",
    "                                heights = (\n",
    "                                    (2e-8, 0), # this step function represents a short spike in current at t = 0.3\n",
    "                                    (0, 0)\n",
    "                                ), times = (\n",
    "                                    0.3, 0.32\n",
    "                                ))\n",
    "    somas = BSSNeuronProcess(name=somas_name, dt=dt, shape=shape)\n",
    "    synapse = BSSSynapseProcess(in_shape=shape, out_shape=shape, synapse_weights = (\n",
    "                                    (0, 0), # this weight matrix specifies that the second dendrite will weigh its first synapse connection\n",
    "                                    (1e-8, 0) # all other weights are 0\n",
    "                                ))\n",
    "    \n",
    "    connect_neurons(dendrites, somas, Direction.BELOW, 8e-8)\n",
    "    somas.spike_out.connect(synapse.synapse_in) # connect all (2) somas to all (2) dendrites, forming a 2x2 synapse matrix\n",
    "    synapse.synapse_out.connect(dendrites.synapse_in)\n",
    "    \n",
    "    dendrites_monitors = StandardNeuronMonitors(dendrites, num_steps) # we'll monitor both types of neurons\n",
    "    somas_monitors = StandardNeuronMonitors(somas, num_steps)\n",
    "    \n",
    "    run_cfg = Loihi1SimCfg(select_tag=model_tag)\n",
    "    somas.run(condition=RunSteps(num_steps=num_steps), run_cfg=run_cfg)\n",
    "    \n",
    "    plot_voltage_and_current('dendrites', [i * dt for i in range(num_steps)], \n",
    "                             dendrites_monitors.V_m_monitor.get_neuron_data(), \n",
    "                             dendrites_monitors.I_monitor.get_neuron_data(), min_voltage_axis = -80e-3, max_voltage_axis=20e-3)\n",
    "    plot_voltage_and_current('somas', [i * dt for i in range(num_steps)], \n",
    "                             somas_monitors.V_m_monitor.get_neuron_data(), \n",
    "                             somas_monitors.I_monitor.get_neuron_data(), min_voltage_axis = -80e-3, max_voltage_axis=20e-3)\n",
    "    \n",
    "    plot_spikes(spikes=[np.where(somas_monitors.spike_monitor.get_neuron_data()[:, i])[0] for i in range(num_neurons)], \n",
    "                legend=[f'{i + 1}' for i in range(num_neurons)], \n",
    "                colors=['#ff0000' for _ in range(num_neurons)])\n",
    "    \n",
    "    somas.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4880a2-f0b2-44ff-b417-166c99b84022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
